{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLnP1cgN6iE2",
        "outputId": "24cdfa3d-91bc-4be5-cc14-a4b4e3579fec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== FORWARD PASS (before weight update) ===\n",
            "H1_net = 0.4125000 => H1 = sigmoid(H1_net) = 0.601687180\n",
            "H2_net = 0.4375000 => H2 = sigmoid(H2_net) = 0.607663170\n",
            "y1_net = 1.416460886 => y1 = sigmoid(y1_net) = 0.804782995\n",
            "y2_net = 1.537395921 => y2 = sigmoid(y2_net) = 0.823085850\n",
            "\n",
            "E1 = 0.5*(T1 - y1)^2 = 0.315840005\n",
            "E2 = 0.5*(T2 - y2)^2 = 0.013930167\n",
            "Total error E_total = E1 + E2 = 0.329770172\n",
            "\n",
            "=== BACKPROP: output layer ===\n",
            "dE/dy1 = 0.794782995, dy1/dnet = 0.157107326, => delta1 = 0.124866231\n",
            "dE/dy2 = -0.166914150, dy2/dnet = 0.145615534, => delta2 = -0.024305293\n",
            "\n",
            "Gradients for hidden->output weights:\n",
            "dw5 (for w5) = delta1 * H1 = 0.075130410\n",
            "dw6 (for w6) = delta1 * H2 = 0.075876610\n",
            "dw7 (for w7) = delta2 * H1 = -0.014624183\n",
            "dw8 (for w8) = delta2 * H2 = -0.014769431\n",
            "\n",
            "Updated hidden->output weights (one step):\n",
            "w5 -> 0.612434795\n",
            "w6 -> 0.662061695\n",
            "w7 -> 0.757312092\n",
            "w8 -> 0.807384716\n",
            "\n",
            "=== BACKPROP: hidden layer ===\n",
            "delta_H1 = (delta1*w5 + delta2*w7) * sigmoid'(H1) = 0.015082763906\n",
            "delta_H2 = (delta1*w6 + delta2*w8) * sigmoid'(H2) = 0.016202758449\n",
            "\n",
            "Gradients for input->hidden weights:\n",
            "dw1 (for w1) = delta_H1 * x1 = 0.000754138195\n",
            "dw2 (for w2) = delta_H1 * x2 = 0.001508276391\n",
            "dw3 (for w3) = delta_H2 * x1 = 0.000810137922\n",
            "dw4 (for w4) = delta_H2 * x2 = 0.001620275845\n",
            "\n",
            "Updated input->hidden weights (one step):\n",
            "w1 -> 0.349622931\n",
            "w2 -> 0.449245862\n",
            "w3 -> 0.549594931\n",
            "w4 -> 0.599189862\n",
            "\n",
            "Updated biases:\n",
            "b1 -> 0.334357239\n",
            "b2 -> 0.549719531\n",
            "\n",
            "=== FORWARD PASS (after weight update) ===\n",
            "H1 (after) = 0.597909681\n",
            "H2 (after) = 0.603903362\n",
            "y1 (after) = 0.788468996\n",
            "y2 (after) = 0.816094198\n",
            "\n",
            "E_total (before) = 0.329770172\n",
            "E_total (after)  = 0.318128603\n",
            "\n",
            "(You should see the total error decreased after one backprop step.)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# -------------------------------\n",
        "# 1) Setup: inputs, initial params\n",
        "# -------------------------------\n",
        "x1 = 0.05\n",
        "x2 = 0.10\n",
        "\n",
        "# Initial weights (input -> hidden)\n",
        "w1, w2 = 0.35, 0.45  # updated weights to H1 from x1, x2\n",
        "w3, w4 = 0.55, 0.60  # updated weights to H2 from x1, x2\n",
        "\n",
        "# Initial weights (hidden -> output)\n",
        "w5, w6 = 0.65, 0.70  # updated weights to y1 from H1, H2\n",
        "w7, w8 = 0.75, 0.80  # updated weights to y2 from H1, H2\n",
        "\n",
        "# Biases\n",
        "b1 = 0.35  # bias for hidden neurons H1, H2\n",
        "b2 = 0.60  # bias for output neurons y1, y2\n",
        "\n",
        "# Targets (desired outputs)\n",
        "T1 = 0.01\n",
        "T2 = 0.99\n",
        "\n",
        "# Learning rate\n",
        "lr = 0.5\n",
        "\n",
        "# -------------------------------\n",
        "# 2) Activation functions\n",
        "# -------------------------------\n",
        "def sigmoid(z):\n",
        "    \"\"\"Sigmoid activation.\"\"\"\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative_from_activation(a):\n",
        "    \"\"\"Derivative of sigmoid given activation a = sigmoid(z): a*(1-a).\"\"\"\n",
        "    return a * (1.0 - a)\n",
        "\n",
        "# -------------------------------\n",
        "# 3) Helper: forward pass\n",
        "# -------------------------------\n",
        "def forward_pass(x1, x2, w1, w2, w3, w4, w5, w6, w7, w8, b1, b2):\n",
        "    \"\"\"\n",
        "    Compute net inputs and activations for hidden and output layers.\n",
        "    Returns a dict with nets and activations for printing/teaching.\n",
        "    \"\"\"\n",
        "    # Hidden layer linear combinations (net inputs)\n",
        "    H1_net = x1 * w1 + x2 * w2 + b1\n",
        "    H2_net = x1 * w3 + x2 * w4 + b1\n",
        "    # Hidden activations (sigmoid)\n",
        "    H1 = sigmoid(H1_net)\n",
        "    H2 = sigmoid(H2_net)\n",
        "    # Output layer linear combinations\n",
        "    y1_net = H1 * w5 + H2 * w6 + b2\n",
        "    y2_net = H1 * w7 + H2 * w8 + b2\n",
        "    # Output activations (sigmoid)\n",
        "    y1 = sigmoid(y1_net)\n",
        "    y2 = sigmoid(y2_net)\n",
        "    return {\n",
        "        \"H1_net\": H1_net, \"H2_net\": H2_net,\n",
        "        \"H1\": H1, \"H2\": H2,\n",
        "        \"y1_net\": y1_net, \"y2_net\": y2_net,\n",
        "        \"y1\": y1, \"y2\": y2\n",
        "    }\n",
        "\n",
        "# -------------------------------\n",
        "# 4) Forward before update (print step-by-step)\n",
        "# -------------------------------\n",
        "print(\"\\n=== FORWARD PASS (before weight update) ===\")\n",
        "out = forward_pass(x1, x2, w1, w2, w3, w4, w5, w6, w7, w8, b1, b2)\n",
        "\n",
        "print(f\"H1_net = {out['H1_net']:.7f} => H1 = sigmoid(H1_net) = {out['H1']:.9f}\")\n",
        "print(f\"H2_net = {out['H2_net']:.7f} => H2 = sigmoid(H2_net) = {out['H2']:.9f}\")\n",
        "print(f\"y1_net = {out['y1_net']:.9f} => y1 = sigmoid(y1_net) = {out['y1']:.9f}\")\n",
        "print(f\"y2_net = {out['y2_net']:.9f} => y2 = sigmoid(y2_net) = {out['y2']:.9f}\")\n",
        "\n",
        "# Compute per-output squared errors and total error\n",
        "E1 = 0.5 * (T1 - out['y1'])**2\n",
        "E2 = 0.5 * (T2 - out['y2'])**2\n",
        "E_total = E1 + E2\n",
        "\n",
        "print(f\"\\nE1 = 0.5*(T1 - y1)^2 = {E1:.9f}\")\n",
        "print(f\"E2 = 0.5*(T2 - y2)^2 = {E2:.9f}\")\n",
        "print(f\"Total error E_total = E1 + E2 = {E_total:.9f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 5) BACKPROPAGATION — output layer\n",
        "#    compute deltas and gradients for w5..w8\n",
        "# -------------------------------\n",
        "print(\"\\n=== BACKPROP: output layer ===\")\n",
        "\n",
        "# For each output neuron i:\n",
        "# delta_i = dE/dy_i * dy_i/dnet_i\n",
        "# where dE/dy_i = -(T_i - y_i) for E = 1/2*(T-y)^2\n",
        "dE_dy1 = -(T1 - out['y1'])\n",
        "dy1_dnet = sigmoid_derivative_from_activation(out['y1'])\n",
        "delta1 = dE_dy1 * dy1_dnet  # scalar\n",
        "\n",
        "dE_dy2 = -(T2 - out['y2'])\n",
        "dy2_dnet = sigmoid_derivative_from_activation(out['y2'])\n",
        "delta2 = dE_dy2 * dy2_dnet  # scalar\n",
        "\n",
        "print(f\"dE/dy1 = {dE_dy1:.9f}, dy1/dnet = {dy1_dnet:.9f}, => delta1 = {delta1:.9f}\")\n",
        "print(f\"dE/dy2 = {dE_dy2:.9f}, dy2/dnet = {dy2_dnet:.9f}, => delta2 = {delta2:.9f}\")\n",
        "\n",
        "# Gradients for weights from hidden -> outputs:\n",
        "dw5 = delta1 * out['H1']\n",
        "dw6 = delta1 * out['H2']\n",
        "dw7 = delta2 * out['H1']\n",
        "dw8 = delta2 * out['H2']\n",
        "\n",
        "print(\"\\nGradients for hidden->output weights:\")\n",
        "print(f\"dw5 (for w5) = delta1 * H1 = {dw5:.9f}\")\n",
        "print(f\"dw6 (for w6) = delta1 * H2 = {dw6:.9f}\")\n",
        "print(f\"dw7 (for w7) = delta2 * H1 = {dw7:.9f}\")\n",
        "print(f\"dw8 (for w8) = delta2 * H2 = {dw8:.9f}\")\n",
        "\n",
        "# Update output weights (gradient descent):\n",
        "w5_new = w5 - lr * dw5\n",
        "w6_new = w6 - lr * dw6\n",
        "w7_new = w7 - lr * dw7\n",
        "w8_new = w8 - lr * dw8\n",
        "\n",
        "print(\"\\nUpdated hidden->output weights (one step):\")\n",
        "print(f\"w5 -> {w5_new:.9f}\")\n",
        "print(f\"w6 -> {w6_new:.9f}\")\n",
        "print(f\"w7 -> {w7_new:.9f}\")\n",
        "print(f\"w8 -> {w8_new:.9f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 6) BACKPROPAGATION — hidden layer\n",
        "#    compute deltas for H1, H2 and gradients for w1..w4\n",
        "# -------------------------------\n",
        "print(\"\\n=== BACKPROP: hidden layer ===\")\n",
        "\n",
        "# Error contribution from both output neurons flows back to each hidden neuron:\n",
        "# delta_H1 = (delta1*w5 + delta2*w7) * sigmoid'(H1_net)\n",
        "# delta_H2 = (delta1*w6 + delta2*w8) * sigmoid'(H2_net)\n",
        "delta_H1 = (delta1 * w5 + delta2 * w7) * sigmoid_derivative_from_activation(out['H1'])\n",
        "delta_H2 = (delta1 * w6 + delta2 * w8) * sigmoid_derivative_from_activation(out['H2'])\n",
        "\n",
        "print(f\"delta_H1 = (delta1*w5 + delta2*w7) * sigmoid'(H1) = {delta_H1:.12f}\")\n",
        "print(f\"delta_H2 = (delta1*w6 + delta2*w8) * sigmoid'(H2) = {delta_H2:.12f}\")\n",
        "\n",
        "# Gradients for input->hidden weights:\n",
        "dw1 = delta_H1 * x1\n",
        "dw2 = delta_H1 * x2\n",
        "dw3 = delta_H2 * x1\n",
        "dw4 = delta_H2 * x2\n",
        "\n",
        "print(\"\\nGradients for input->hidden weights:\")\n",
        "print(f\"dw1 (for w1) = delta_H1 * x1 = {dw1:.12f}\")\n",
        "print(f\"dw2 (for w2) = delta_H1 * x2 = {dw2:.12f}\")\n",
        "print(f\"dw3 (for w3) = delta_H2 * x1 = {dw3:.12f}\")\n",
        "print(f\"dw4 (for w4) = delta_H2 * x2 = {dw4:.12f}\")\n",
        "\n",
        "# Update hidden weights w1_new = w1 - lr * dw1 w2_new = w2 - lr * dw2 w3_new = w3 - lr * dw3 w4_new = w4 - lr * dw4\n",
        "w1_new = w1 - lr * dw1\n",
        "w2_new = w2 - lr * dw2\n",
        "w3_new = w3 - lr * dw3\n",
        "w4_new = w4 - lr * dw4\n",
        "\n",
        "print(\"\\nUpdated input->hidden weights (one step):\")\n",
        "print(f\"w1 -> {w1_new:.9f}\")\n",
        "print(f\"w2 -> {w2_new:.9f}\")\n",
        "print(f\"w3 -> {w3_new:.9f}\")\n",
        "print(f\"w4 -> {w4_new:.9f}\")\n",
        "\n",
        "# For completeness, update biases too (if you want)\n",
        "b2_new = b2 - lr * (delta1 + delta2)  # update using both output deltas\n",
        "b1_new = b1 - lr * (delta_H1 + delta_H2)  # update using hidden deltas\n",
        "\n",
        "print(f\"\\nUpdated biases:\")\n",
        "print(f\"b1 -> {b1_new:.9f}\")\n",
        "print(f\"b2 -> {b2_new:.9f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 7) Forward pass after the update (to show error decreased)\n",
        "# -------------------------------\n",
        "print(\"\\n=== FORWARD PASS (after weight update) ===\")\n",
        "out_after = forward_pass(x1, x2,\n",
        "    w1_new, w2_new, w3_new, w4_new,\n",
        "    w5_new, w6_new, w7_new, w8_new,\n",
        "    b1_new, b2_new)\n",
        "\n",
        "print(f\"H1 (after) = {out_after['H1']:.9f}\")\n",
        "print(f\"H2 (after) = {out_after['H2']:.9f}\")\n",
        "print(f\"y1 (after) = {out_after['y1']:.9f}\")\n",
        "print(f\"y2 (after) = {out_after['y2']:.9f}\")\n",
        "\n",
        "E1_after = 0.5 * (T1 - out_after['y1'])**2\n",
        "E2_after = 0.5 * (T2 - out_after['y2'])**2\n",
        "E_total_after = E1_after + E2_after\n",
        "\n",
        "print(f\"\\nE_total (before) = {E_total:.9f}\")\n",
        "print(f\"E_total (after)  = {E_total_after:.9f}\")\n",
        "print(\"\\n(You should see the total error decreased after one backprop step.)\")"
      ]
    }
  ]
}